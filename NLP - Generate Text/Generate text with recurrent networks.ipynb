{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eadd0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.9966180324554443\n",
      "Epoch: 100, Loss: 0.840171217918396\n",
      "Epoch: 200, Loss: 0.28384754061698914\n",
      "Epoch: 300, Loss: 0.07948210090398788\n",
      "Epoch: 400, Loss: 0.032600242644548416\n",
      "Epoch: 500, Loss: 0.017976846545934677\n",
      "Epoch: 600, Loss: 0.011216194368898869\n",
      "Epoch: 700, Loss: 0.00785580463707447\n",
      "Epoch: 800, Loss: 0.005819302052259445\n",
      "Epoch: 900, Loss: 0.004371980205178261\n",
      "Epoch: 1000, Loss: 0.0034466085489839315\n",
      "Epoch: 1100, Loss: 0.0027934431564062834\n",
      "Epoch: 1200, Loss: 0.0022935373708605766\n",
      "Epoch: 1300, Loss: 0.0019028299720957875\n",
      "Epoch: 1400, Loss: 0.0015955442795529962\n",
      "Epoch: 1500, Loss: 0.001346526900306344\n",
      "Epoch: 1600, Loss: 0.001144013600423932\n",
      "Epoch: 1700, Loss: 0.0009799095569178462\n",
      "Epoch: 1800, Loss: 0.0008454998023808002\n",
      "Epoch: 1900, Loss: 0.0007344027399085462\n",
      "This is an example sentence..oo trrinnnnc...noh  ssntence for training...nooh  sentence for training...noot\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "# Define the tokenizer\n",
    "def char_tokenizer(text):\n",
    "    return list(text)\n",
    "\n",
    "# Load dataset (Example placeholder, replace with actual data loading code)\n",
    "train_dataset = [(\"This is an example sentence.\", 0), (\"Another sentence for training.\", 1)]\n",
    "counter = Counter()\n",
    "for (line, label) in train_dataset:\n",
    "    counter.update(char_tokenizer(line))\n",
    "vocab_obj = vocab(counter)\n",
    "\n",
    "# Define LSTM-based generative network\n",
    "class LSTMGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, n_layers=1):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(vocab_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "# Helper function to encode text\n",
    "def one_hot_encode(text, vocab_obj):\n",
    "    tensor = torch.zeros(len(text), len(vocab_obj))\n",
    "    for i, char in enumerate(text):\n",
    "        tensor[i][vocab_obj[char]] = 1\n",
    "    return tensor\n",
    "\n",
    "# Define training parameters\n",
    "vocab_size = len(vocab_obj)\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "n_epochs = 2000\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LSTMGenerator(vocab_size, hidden_dim, n_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for (line, label) in train_dataset:\n",
    "        hidden = model.init_hidden(1)\n",
    "        model.zero_grad()\n",
    "        input_seq = one_hot_encode(line[:-1], vocab_obj).unsqueeze(0)\n",
    "        target_seq = torch.tensor([vocab_obj[char] for char in line[1:]])\n",
    "        \n",
    "        output, hidden = model(input_seq, hidden)\n",
    "        loss = criterion(output.squeeze(), target_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, start_text, length=100):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    input_seq = one_hot_encode(start_text, vocab_obj).unsqueeze(0)\n",
    "    chars = [ch for ch in start_text]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            char_index = topi[0, -1].item()\n",
    "            char = vocab_obj.lookup_token(char_index)\n",
    "            chars.append(char)\n",
    "            input_seq = one_hot_encode(chars[-1], vocab_obj).unsqueeze(0)\n",
    "    \n",
    "    return ''.join(chars)\n",
    "\n",
    "# Generate and print text\n",
    "print(generate_text(model, start_text=\"This is\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d69ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
